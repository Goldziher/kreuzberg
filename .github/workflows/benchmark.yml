name: Performance Benchmarks

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
    types: [opened, synchronize, reopened]
  schedule:
    # Run benchmarks weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:
    inputs:
      include_stress_tests:
        description: 'Include stress tests'
        required: false
        default: 'false'
        type: boolean
      flame_graphs:
        description: 'Generate flame graphs'
        required: false
        default: 'false'
        type: boolean

jobs:
  benchmark:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ${{ github.event_name == 'pull_request' && fromJSON('["3.13"]') || fromJSON('["3.9", "3.10", "3.11", "3.12", "3.13"]') }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Set up Python ${{ matrix.python-version }}
        run: uv python install ${{ matrix.python-version }}

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y pandoc tesseract-ocr

      - name: Install dependencies
        run: |
          # Install main package with extras
          uv pip install -e ".[all]"
          # Install benchmarks package
          uv pip install -e ./benchmarks

      - name: Create test files
        run: |
          mkdir -p test_files
          echo "Sample text content for benchmarking" > test_files/sample.txt
          # Create a simple PDF for testing
          echo '%PDF-1.4
          1 0 obj << /Type /Catalog /Pages 2 0 R >> endobj
          2 0 obj << /Type /Pages /Kids [3 0 R] /Count 1 >> endobj
          3 0 obj << /Type /Page /Parent 2 0 R /MediaBox [0 0 612 792] /Contents 4 0 R >> endobj
          4 0 obj << /Length 44 >> stream
          BT /F1 12 Tf 100 700 Td (Benchmark Test) Tj ET
          endstream endobj
          xref
          0 5
          0000000000 65535 f
          0000000009 00000 n
          0000000074 00000 n
          0000000131 00000 n
          0000000252 00000 n
          trailer << /Size 5 /Root 1 0 R >>
          startxref
          296
          %%EOF' > test_files/benchmark.pdf

      - name: Run benchmarks
        run: |
          # Run comparison benchmarks (sync vs async)
          uv run python -m kreuzberg_benchmarks.cli run \
            --comparison-only \
            --test-files-dir test_files \
            --output-dir benchmark_results \
            --suite-name "ci_sync_vs_async" \
            ${{ github.event.inputs.flame_graphs == 'true' && '--flame' || '' }}

      - name: Run stress tests (if requested)
        if: github.event.inputs.include_stress_tests == 'true' || github.event_name == 'schedule'
        run: |
          uv run python -m kreuzberg_benchmarks.cli run \
            --stress \
            --test-files-dir test_files \
            --output-dir benchmark_results \
            --suite-name "ci_stress_tests"

      - name: Archive benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: benchmark_results/
          retention-days: 30

      - name: Comment benchmark results on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            // Read the latest benchmark results
            const resultsPath = 'benchmark_results/latest.json';
            if (!fs.existsSync(resultsPath)) {
              console.log('No benchmark results found');
              return;
            }

            const results = JSON.parse(fs.readFileSync(resultsPath, 'utf8'));

            // Format results for PR comment
            const formatDuration = (seconds) => {
              if (seconds < 1) return `${(seconds * 1000).toFixed(1)}ms`;
              return `${seconds.toFixed(3)}s`;
            };

            const formatMemory = (bytes) => {
              const mb = bytes / (1024 * 1024);
              return `${mb.toFixed(1)}MB`;
            };

            let comment = `## üöÄ Performance Benchmark Results\n\n`;
            comment += `**Suite:** ${results.suite_name}\n`;
            comment += `**Total Duration:** ${formatDuration(results.summary.total_time)}\n`;
            comment += `**Peak Memory:** ${formatMemory(results.summary.peak_memory)}\n`;
            comment += `**Success Rate:** ${results.summary.success_rate}%\n\n`;

            comment += `### Individual Benchmarks\n\n`;
            comment += `| Benchmark | Status | Duration | Memory Peak | CPU Avg |\n`;
            comment += `|-----------|--------|----------|-------------|----------|\n`;

            for (const bench of results.benchmarks) {
              const status = bench.success ? '‚úÖ' : '‚ùå';
              const duration = formatDuration(bench.duration);
              const memory = formatMemory(bench.memory_peak);
              const cpu = bench.cpu_avg ? `${bench.cpu_avg.toFixed(1)}%` : 'N/A';

              comment += `| ${bench.name} | ${status} | ${duration} | ${memory} | ${cpu} |\n`;
            }

            comment += `\n**System:** ${results.system_info.platform} (${results.system_info.cpu_count} cores)\n`;
            comment += `**Python:** ${results.system_info.python_version}\n\n`;
            comment += `<details><summary>View detailed results</summary>\n\n`;
            comment += `\`\`\`json\n${JSON.stringify(results, null, 2)}\n\`\`\`\n\n</details>`;

            // Post comment to PR
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

      - name: Check performance regression
        if: github.event_name == 'pull_request'
        run: |
          # This could be enhanced to compare against baseline performance
          # For now, just ensure benchmarks completed successfully
          if [ ! -f "benchmark_results/latest.json" ]; then
            echo "‚ùå Benchmark results not found"
            exit 1
          fi

          # Check if any benchmarks failed
          FAILED_COUNT=$(uv run python -c "
          import json
          with open('benchmark_results/latest.json') as f:
              data = json.load(f)
          failed = sum(1 for b in data['benchmarks'] if not b['success'])
          print(failed)
          ")

          if [ "$FAILED_COUNT" -gt 0 ]; then
            echo "‚ùå $FAILED_COUNT benchmark(s) failed"
            exit 1
          fi

          echo "‚úÖ All benchmarks passed successfully"

  # Store baseline benchmarks for main branch
  store-baseline:
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    runs-on: ubuntu-latest
    needs: benchmark

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: benchmark_results/

      - name: Store baseline in repository
        run: |
          mkdir -p .github/benchmarks
          cp benchmark_results/latest.json .github/benchmarks/baseline.json

      - name: Commit baseline
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add .github/benchmarks/baseline.json
          git commit -m "chore: update performance baseline [skip ci]" || exit 0
          git push
