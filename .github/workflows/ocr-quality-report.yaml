name: OCR Quality Report

on:
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.event.workflow_run.id || github.run_id }}
  cancel-in-progress: false

jobs:
  quality-report:
    name: Generate Shard ${{ matrix.shard }}
    runs-on: ubuntu-latest
    timeout-minutes: 120
    strategy:
      fail-fast: false
      matrix:
        shard: [0, 1, 2, 3, 4]
    env:
      UV_PROJECT_ENVIRONMENT: .venv
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5
        with:
          submodules: recursive

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true

      - name: Set up Python
        id: setup-python
        uses: actions/setup-python@v6
        with:
          python-version: "3.13"

      - name: Cache uv environment
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            .venv
          key: ocr-quality-${{ runner.os }}-${{ steps.setup-python.outputs.python-version }}-${{ hashFiles('uv.lock') }}
          restore-keys: |
            ocr-quality-${{ runner.os }}-

      - name: Sync dependencies
        run: uv sync --all-extras --all-packages --all-groups

      - name: Run quality report shard
        run: |
          mkdir -p results
          uv run python scripts/ocr_quality_report.py \
            --use-ground-truth \
            --shard-count 5 \
            --shard-index ${{ matrix.shard }} \
            --output results/ocr_quality_shard_${{ matrix.shard }}.json

      - name: Upload shard artifact
        uses: actions/upload-artifact@v4
        with:
          name: ocr-quality-shard-${{ matrix.shard }}
          path: results/ocr_quality_shard_${{ matrix.shard }}.json
          retention-days: 14

  merge-quality-report:
    name: Merge Quality Report
    runs-on: ubuntu-latest
    needs: quality-report
    timeout-minutes: 30
    env:
      UV_PROJECT_ENVIRONMENT: .venv
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true

      - name: Set up Python
        id: setup-python
        uses: actions/setup-python@v6
        with:
          python-version: "3.13"

      - name: Cache uv environment
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            .venv
          key: ocr-quality-merge-${{ runner.os }}-${{ steps.setup-python.outputs.python-version }}-${{ hashFiles('uv.lock') }}
          restore-keys: |
            ocr-quality-merge-${{ runner.os }}-

      - name: Download shard artifacts
        uses: actions/download-artifact@v4
        with:
          path: results/shards

      - name: Merge shard outputs with jq
        run: |
          set -eo pipefail
          shopt -s globstar
          jq -s '{configs: [.[].config], results: (reduce .[] as $doc ([]; . + $doc.results)), worst: (reduce .[] as $doc ([]; . + $doc.worst_offenders))}' \
            results/shards/**/*.json > results/ocr_quality_combined_partial.json

      - name: Recompute summary
        run: |
          uv run python - <<'PY'
          import json
          from pathlib import Path

          combined_path = Path("results/ocr_quality_combined_partial.json")
          data = json.loads(combined_path.read_text())
          results = data.get("results", [])

          def mean(values):
              return sum(values) / len(values) if values else None

          text_f1 = [
              res["text_score"]["f1"]
              for res in results
              if res.get("text_score") and res["text_score"] and res["text_score"].get("f1") is not None
          ]
          numeric_f1 = [
              res["numeric_score"]["f1"]
              for res in results
              if res.get("numeric_score") and res["numeric_score"] and res["numeric_score"].get("f1") is not None
          ]
          markdown_f1 = [
              res["markdown_score"]["f1"]
              for res in results
              if res.get("markdown_score") and res["markdown_score"] and res["markdown_score"].get("f1") is not None
          ]
          layout_deltas = [res["layout_delta"] for res in results if res.get("layout_delta") is not None]
          structure_deltas = [
              res["markdown_structure_delta"]
              for res in results
              if res.get("markdown_structure_delta") is not None
          ]

          summary = {
              "documents_evaluated": len(results),
              "text_f1_avg": mean(text_f1),
              "text_f1_min": min(text_f1) if text_f1 else None,
              "text_f1_max": max(text_f1) if text_f1 else None,
              "numeric_f1_avg": mean(numeric_f1),
              "markdown_f1_avg": mean(markdown_f1),
              "layout_delta_avg": mean(layout_deltas),
              "layout_delta_max": max(layout_deltas) if layout_deltas else None,
              "markdown_structure_delta_avg": mean(structure_deltas),
              "markdown_structure_delta_max": max(structure_deltas) if structure_deltas else None,
          }

          scored = [
              res
              for res in results
              if res.get("text_score") and res["text_score"] and res["text_score"].get("f1") is not None
          ]
          scored.sort(key=lambda res: res["text_score"]["f1"])

          worst = [
              {
                  "doc_id": res["doc_id"],
                  "pdf_path": res["pdf_path"],
                  "text_f1": res["text_score"]["f1"] if res.get("text_score") else None,
                  "layout_delta": res.get("layout_delta"),
                  "markdown_f1": res["markdown_score"]["f1"] if res.get("markdown_score") else None,
                  "markdown_structure_delta": res.get("markdown_structure_delta"),
              }
              for res in scored[:5]
          ]

          configs = data.get("configs", [])
          base_config = dict(configs[0]) if configs else {}
          base_config.pop("shard_index", None)
          base_config.pop("shard_count", None)
          base_config["merged_shards"] = [cfg.get("shard_index") for cfg in configs]
          base_config["merged_shard_count"] = len(configs)

          final = {
              "config": base_config,
              "summary": summary,
              "worst_offenders": worst,
              "results": results,
          }

          output_path = Path("results/ocr_quality_report.json")
          output_path.write_text(json.dumps(final, indent=2))
          PY

      - name: Upload merged report
        uses: actions/upload-artifact@v4
        with:
          name: ocr-quality-report
          path: results/ocr_quality_report.json
          retention-days: 30

      - name: Show summary
        run: |
          jq '{summary, worst_offenders: .worst_offenders}' results/ocr_quality_report.json
