name: Kreuzberg Performance Benchmarks

on:
  schedule:
    - cron: '0 2 * * *'
  workflow_dispatch:

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    strategy:
      matrix:
        python-version: ["3.13"]

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v5

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true
          cache-dependency-glob: uv.lock

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            tesseract-ocr \
            tesseract-ocr-eng \
            poppler-utils \
            libmagic1 \
            pandoc

      - name: Install Python dependencies
        run: |
          uv sync --all-packages --all-extras --all-groups

      - name: Run benchmark tests
        env:
          RUN_BENCHMARKS: "1"
        run: |
          uv run pytest tests/benchmarks/ \
            --benchmark-only \
            --benchmark-json=benchmark_results.json \
            --benchmark-columns=min,max,mean,stddev,median,iqr \
            --benchmark-sort=mean \
            --benchmark-disable-gc \
            --benchmark-warmup=on \
            --benchmark-warmup-iterations=1 \
            -v

      - name: Generate benchmark report
        if: always()
        run: |
          uv run python -c "
          import json
          from pathlib import Path

          results_file = Path('benchmark_results.json')
          if results_file.exists():
              with open(results_file) as f:
                  data = json.load(f)

              print('## Benchmark Results Summary\n')
              print(f'Python {data.get('machine_info', {}).get('python_version', 'unknown')}')
              print(f'Platform: {data.get('machine_info', {}).get('system', 'unknown')}\n')

              if 'benchmarks' in data:
                  print('| Test | Mean (ms) | StdDev | Min | Max |')
                  print('|------|-----------|--------|-----|-----|')
                  for bench in data['benchmarks'][:10]:  # Top 10
                      stats = bench.get('stats', {})
                      name = bench.get('name', 'unknown')
                      # Shorten the name for readability
                      if '[' in name:
                          name = name.split('[')[1].split(']')[0][:30]
                      print(f'| {name} | {stats.get('mean', 0)*1000:.2f} | {stats.get('stddev', 0)*1000:.2f} | {stats.get('min', 0)*1000:.2f} | {stats.get('max', 0)*1000:.2f} |')
          "

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-py${{ matrix.python-version }}-${{ github.sha }}
          path: benchmark_results.json
          retention-days: 30

      - name: Store benchmark results
        if: always()
        run: |
          # Store results for historical comparison
          echo "Results stored in artifact"
