name: Comparative Framework Benchmark

on:
  push:
    branches:
      - main
  workflow_dispatch:
    inputs:
      iterations:
        description: "Number of iterations per test"
        required: false
        default: "3"
        type: string
      timeout:
        description: "Timeout per file in seconds"
        required: false
        default: "300"
        type: string

jobs:
  # Dummy job that succeeds on push to prevent workflow failure
  skip-on-push:
    if: github.event_name == 'push'
    runs-on: ubuntu-latest
    steps:
      - name: Skip benchmark on push
        run: echo "Comparative benchmarks only run on manual dispatch"

  comparative-benchmark:
    # Only run on manual dispatch to avoid expensive automated runs
    if: github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    timeout-minutes: 360

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true
          cache-dependency-glob: |
            uv.lock
            benchmarks/uv.lock

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"

      - name: Cache uv dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            .venv
            benchmarks/.venv
          key: uv-${{ runner.os }}-${{ hashFiles('uv.lock', 'benchmarks/uv.lock') }}
          restore-keys: |
            uv-${{ runner.os }}-

      - name: Cache APT packages
        uses: awalsh128/cache-apt-pkgs-action@latest
        with:
          packages: tesseract-ocr tesseract-ocr-eng tesseract-ocr-deu tesseract-ocr-fra tesseract-ocr-heb tesseract-ocr-chi-sim tesseract-ocr-jpn tesseract-ocr-kor poppler-utils libmagic1 pandoc
          version: 2.0

      - name: Install Python Dependencies
        uses: nick-fields/retry@v3
        with:
          timeout_minutes: 5
          max_attempts: 3
          retry_wait_seconds: 30
          command: |
            echo "Installing entire workspace with all packages and extras..."
            # Install everything for benchmarks - all extras, all packages, all groups
            uv sync --all-extras --all-packages --all-groups
          shell: bash

      - name: Clear Kreuzberg Cache
        run: |
          echo "Clearing any existing Kreuzberg cache..."
          rm -rf ~/.kreuzberg .kreuzberg benchmarks/.kreuzberg
          echo "Cache cleared"

      - name: Run Benchmarks
        id: benchmark
        run: |
          cd benchmarks

          ITERATIONS="${{ github.event.inputs.iterations }}"
          TIMEOUT="${{ github.event.inputs.timeout }}"

          echo "=== Starting Benchmark Suite ==="
          echo "Iterations: $ITERATIONS"
          echo "Timeout: ${TIMEOUT}s per file"
          echo "Running all frameworks and categories"

          # Run benchmarks and generate aggregated JSON directly
          uv run python -m src.cli \
            --iterations "$ITERATIONS" \
            --timeout "$TIMEOUT" \
            --output aggregated-results/aggregated_results.json

          echo "Benchmark execution completed"

      - name: Generate Visualizations
        if: always()
        run: |
          echo "=== Generating Visualizations ==="
          mkdir -p docs/benchmarks/charts

          if [ -f "benchmarks/aggregated-results/aggregated_results.json" ]; then
            uv run scripts/generate_benchmark_visualizations.py \
              --input benchmarks/aggregated-results/aggregated_results.json \
              --output docs/benchmarks/charts

            echo "✅ Visualizations generated successfully"
          else
            echo "⚠️ No results for visualization"
          fi

      - name: Generate Documentation
        if: always()
        run: |
          echo "=== Generating Benchmark Documentation ==="

          if [ -f "benchmarks/aggregated-results/aggregated_results.json" ]; then
            uv run scripts/generate_benchmark_docs.py \
              --input benchmarks/aggregated-results/aggregated_results.json \
              --output docs/benchmarks \
              --charts docs/benchmarks/charts

            echo "✅ Documentation generated successfully"
          else
            echo "⚠️ No results for documentation"
          fi

      - name: Create Summary
        if: always()
        run: |
          echo "## 📊 Comparative Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### Configuration" >> $GITHUB_STEP_SUMMARY
          echo "- **Iterations**: ${{ github.event.inputs.iterations }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Timeout**: ${{ github.event.inputs.timeout }}s" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Add summary from aggregated results if available
          if [ -f "benchmarks/aggregated-results/aggregated_results.json" ]; then
            echo "### Results Summary" >> $GITHUB_STEP_SUMMARY
            python -c 'import json; data=json.load(open("benchmarks/aggregated-results/aggregated_results.json")); print(f"- **Total Files**: {data.get(\"total_files_processed\", 0)}") if "error" not in data else print("❌ " + data.get("error", "Unknown error"))' >> $GITHUB_STEP_SUMMARY 2>/dev/null || echo "⚠️ Could not parse results" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ No results file found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload Aggregated Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.run_id }}
          path: benchmarks/aggregated-results/
          retention-days: 90

      - name: Upload Documentation
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-docs-${{ github.run_id }}
          path: docs/benchmarks/
          retention-days: 90
