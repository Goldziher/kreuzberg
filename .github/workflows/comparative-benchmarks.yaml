name: Comparative Framework Benchmarks

on:
  workflow_dispatch:
    inputs:
      frameworks:
        description: "Comma-separated frameworks to benchmark (docling,extractous,markitdown,unstructured,kreuzberg)"
        required: false
        default: "docling,extractous,markitdown,unstructured,kreuzberg"
        type: string
      timeout:
        description: "Timeout per extraction in seconds"
        required: false
        default: "300"
        type: string

jobs:
  benchmark:
    if: github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v5

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true
          cache-dependency-glob: |
            uv.lock
            pyproject.toml

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.13"

      - name: Setup Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          components: rustfmt, clippy

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: benchmark-${{ runner.os }}-${{ hashFiles('uv.lock', 'Cargo.lock') }}
          restore-keys: |
            benchmark-${{ runner.os }}-

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            tesseract-ocr \
            tesseract-ocr-eng \
            tesseract-ocr-deu \
            tesseract-ocr-fra \
            poppler-utils \
            libmagic1 \
            pandoc

      - name: Install Python dependencies
        run: |
          echo "Installing workspace with comparative-benchmarks group..."
          uv sync --group comparative-benchmarks

      - name: Build benchmark harness
        run: |
          cd tools/benchmark-harness
          cargo build --release --features extractous-native

      - name: Validate fixtures
        run: |
          cd tools/benchmark-harness
          cargo run --release -- validate --fixtures fixtures/
          echo "âœ“ All fixtures validated successfully"

      - name: Run benchmarks
        id: benchmark
        continue-on-error: true
        run: |
          cd tools/benchmark-harness

          FRAMEWORKS="${{ github.event.inputs.frameworks }}"
          TIMEOUT="${{ github.event.inputs.timeout }}"

          echo "=== Starting Comparative Benchmarks ==="
          echo "Frameworks: $FRAMEWORKS"
          echo "Timeout: ${TIMEOUT}s per extraction"
          echo "Fixtures: $(ls fixtures/*.json | wc -l) total"

          # Create results directory
          mkdir -p results

          # Convert framework input to use correct adapter names
          ADAPTER_FRAMEWORKS=$(echo "$FRAMEWORKS" | sed 's/kreuzberg/kreuzberg-native/g' | sed 's/extractous/extractous-python/g')

          echo "Running benchmarks with adapters: $ADAPTER_FRAMEWORKS"

          # Run actual benchmarks with all fixtures
          ./target/release/benchmark-harness run \
            --fixtures fixtures/ \
            --frameworks "$ADAPTER_FRAMEWORKS" \
            --output results \
            --timeout "$TIMEOUT" \
            --iterations 3 \
            --warmup 1 \
            --max-concurrent 4 \
            --mode batch 2>&1 | tee results/benchmark_run.log

          echo "Benchmark execution completed"

          # Display summary
          if [ -f results/summary.json ]; then
            echo "Summary results generated:"
            cat results/summary.json
          fi

      - name: Generate summary
        if: always()
        run: |
          echo "## ðŸ“Š Comparative Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Configuration" >> $GITHUB_STEP_SUMMARY
          echo "- **Frameworks**: ${{ github.event.inputs.frameworks }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Timeout**: ${{ github.event.inputs.timeout }}s" >> $GITHUB_STEP_SUMMARY
          echo "- **Fixtures**: $(ls tools/benchmark-harness/fixtures/*.json 2>/dev/null | wc -l)" >> $GITHUB_STEP_SUMMARY
          echo "- **Iterations**: 3 (with 1 warmup)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f "tools/benchmark-harness/results/summary.json" ]; then
            echo "### Benchmark Summary" >> $GITHUB_STEP_SUMMARY
            echo '```json' >> $GITHUB_STEP_SUMMARY
            cat tools/benchmark-harness/results/summary.json >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

          if [ -f "tools/benchmark-harness/results/benchmark_run.log" ]; then
            echo "### Execution Log (last 100 lines)" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -100 tools/benchmark-harness/results/benchmark_run.log >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ No benchmark results found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.run_id }}
          path: |
            tools/benchmark-harness/results/
            tools/benchmark-harness/target/release/
          retention-days: 30

      - name: Upload test output
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-output-${{ github.run_id }}
          path: tools/benchmark-harness/results/*.log
          retention-days: 30
