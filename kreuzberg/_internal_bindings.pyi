from typing import Any

class CacheStats:
    total_files: int
    total_size_mb: float
    available_space_mb: float
    oldest_file_age_days: float
    newest_file_age_days: float

def generate_cache_key(**kwargs: Any) -> str: ...
def batch_generate_cache_keys(items: list[Any]) -> list[str]: ...
def fast_hash(data: bytes) -> int: ...
def validate_cache_key(key: str) -> bool: ...
def filter_old_cache_entries(cache_times: list[float], current_time: float, max_age_seconds: float) -> list[int]: ...
def sort_cache_by_access_time(entries: list[tuple[str, float]]) -> list[str]: ...
def get_available_disk_space(path: str) -> float: ...
def get_cache_metadata(cache_dir: str) -> CacheStats: ...
def cleanup_cache(
    cache_dir: str, max_age_days: float, max_size_mb: float, target_size_ratio: float
) -> tuple[int, float]: ...
def smart_cleanup_cache(
    cache_dir: str, max_age_days: float, max_size_mb: float, min_free_space_mb: float
) -> tuple[int, float]: ...
def is_cache_valid(cache_path: str, max_age_days: float) -> bool: ...
def clear_cache_directory(cache_dir: str) -> tuple[int, float]: ...
def batch_cleanup_caches(
    cache_dirs: list[str], max_age_days: float, max_size_mb: float, min_free_space_mb: float
) -> list[tuple[int, float]]: ...
def calculate_quality_score(text: str, ocr_confidence: float | None, has_tables: bool, has_images: bool) -> float: ...
def clean_extracted_text(text: str) -> str: ...
def normalize_spaces(text: str) -> str: ...
def safe_decode(data: bytes) -> str: ...
def batch_process_texts(texts: list[bytes]) -> list[str]: ...
def calculate_text_confidence(text: str) -> float: ...
def fix_mojibake(text: str) -> str: ...
def get_encoding_cache_key(data: bytes) -> str: ...
def normalize_image_dpi(image_data: bytes, target_dpi: int, current_dpi: int | None) -> bytes: ...
def batch_normalize_images(images: list[bytes], target_dpi: int) -> list[bytes]: ...
def reduce_tokens(text: str, reduction_level: str) -> str: ...
def batch_reduce_tokens(texts: list[str], reduction_level: str) -> list[str]: ...
def get_reduction_statistics(original: str, reduced: str) -> dict[str, Any]: ...
def table_from_arrow_to_markdown(arrow_bytes: bytes) -> str: ...

# Excel processing functions
def read_excel_file(file_path: str) -> ExcelWorkbook: ...
def read_excel_bytes(data: bytes, file_extension: str) -> ExcelWorkbook: ...
def excel_to_markdown(file_path: str) -> str: ...
def benchmark_excel_reading(file_path: str, iterations: int) -> float: ...

class ImagePreprocessingMetadata: ...
class ExtractionConfig: ...
class TokenReductionConfig: ...
class ReductionLevel: ...

class ExcelWorkbook:
    sheets: list[ExcelSheet]
    metadata: dict[str, str]

class ExcelSheet:
    name: str
    markdown: str
    row_count: int
    col_count: int
    cell_count: int
